{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a02078-96b3-4f14-9669-fb49f4edb4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import TensorDataset\n",
    "import argparse\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be49837-23e5-418a-a1f6-aa834f8bb18b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from loader import ICUVariableLengthLoaderTables, ICUVariableLengthDataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c014230b-cda3-4cbc-920a-2d5b16570858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from net_dev import GNNStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f5d7406-8f67-45bd-add7-cb4a8fe7a97b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608f5b7-259a-4fea-be59-b90be979c3b9",
   "metadata": {},
   "source": [
    "## arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7e2de2d-8c0d-4ab8-8090-5e5621f13034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'arch': 'dyGIN2d', #what other models I can put here?? dyGCN2d, dyGIN2d\n",
    "    'dataset': 'Resp_failure_development_attentionMSE', # this will logged into the logs folder\n",
    "    'num_layers': 2,  # the number of GNN layers  3\n",
    "    'groups': 32,  # the number of time series groups (num_graphs)\n",
    "    'pool_ratio': 0.0,  # the ratio of pooling for nodes # initially 0.1 but changed to 0 because the node number was decreasing\n",
    "    'kern_size': [3,3],  # list of time conv kernel size for each layer [9,5,3]\n",
    "    'in_dim': 64,  # input dimensions of GNN stacks\n",
    "    'hidden_dim': 64,  # hidden dimensions of GNN stacks\n",
    "    'out_dim': 64,  # output dimensions of GNN stacks\n",
    "    'workers': 0,  # number of data loading workers\n",
    "    'epochs': 20,  # number of total epochs to run\n",
    "    'batch_size': 8,  # mini-batch size, this is the total batch size of all GPUs\n",
    "    'val_batch_size': 8,  # validation batch size\n",
    "    'lr': 0.0002,  # initial learning rate\n",
    "    'weight_decay': 1e-4,  # weight decay\n",
    "    'evaluate': False,  # evaluate model on validation set\n",
    "    'seed': 3,  # seed for initializing training\n",
    "    'gpu': 0,  # GPU id to use\n",
    "    'use_benchmark': True,  # use benchmark\n",
    "    'tag': 'date',  # the tag for identifying the log and model files\n",
    "    'loss':'bce',\n",
    "    'resample':'5min',\n",
    "    'series_length':2016,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c9a44f-2214-47c8-8de8-d47d7ba5202d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "              \n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        \n",
    "        # print(output, target)\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            try:\n",
    "                res.append(correct_k.mul_(100.0 / batch_size))\n",
    "            except:\n",
    "                res.append(0)\n",
    "\n",
    "        return res\n",
    "\n",
    "def log_msg(message, log_file):\n",
    "    with open(log_file, 'a') as f:\n",
    "        print(message, file=f)\n",
    "\n",
    "\n",
    "def get_default_train_val_test_loader(args):\n",
    "\n",
    "    dsid = args['dataset']\n",
    "    h5_path= 'h5_folder/ml_stage_12h.h5' # We are not authorized to share the dataset. Please contact the authors of Hirid-ICU-benchmark for the data\n",
    "    \n",
    "    num_nodes = 231\n",
    "    seq_length = 864 #288 #2016 ### now we will reduce the sequence length to see the performance in short-range\n",
    "    num_classes = 2\n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    task   = 'Dynamic_RespFailure_12Hours' ##27249\n",
    "    maxlen            = 2016 #288 #2016                                                                             \n",
    "    data_loader_train = ICUVariableLengthDataset(source_path=h5_path, maxlen=maxlen, task=task, split='train') \n",
    "    data_loader_val   = ICUVariableLengthDataset(source_path=h5_path, maxlen=maxlen, task=task, split='val')   \n",
    "    data_loader_test  = ICUVariableLengthDataset(source_path=h5_path, maxlen=maxlen, task=task, split='test')  \n",
    "    \n",
    "    train_loader = DataLoader(data_loader_train, batch_size=4, shuffle=True, num_workers=1,pin_memory=True, prefetch_factor=2)\n",
    "    val_loader   = DataLoader(data_loader_val,   batch_size=4, shuffle=False, num_workers=1,pin_memory=True, prefetch_factor=2)\n",
    "    test_loader  = DataLoader(data_loader_test,  batch_size=4, shuffle=False, num_workers=1,pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, num_nodes, seq_length, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4305aaf-43d3-4f6b-b453-f4fc2f4aaa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_test_loader(args):\n",
    "\n",
    "    # get dataset-id\n",
    "    dsid = args['dataset']\n",
    "    h5_path= 'h5_folder/ml_stage_12h.h5'  # We are not authorized to share the dataset. Please contact the authors of Hirid-ICU-benchmark for the data\n",
    "    \n",
    "    num_nodes = 231\n",
    "    seq_length = 2016\n",
    "    num_classes = 2\n",
    "   \n",
    "    \n",
    "    task   = 'Dynamic_RespFailure_12Hours' \n",
    "    maxlen            = 2016                                                         \n",
    "    data_loader_test  = ICUVariableLengthDataset(source_path=h5_path, maxlen=maxlen, task=task, split='test')\n",
    "    \n",
    "    test_loader  = DataLoader(data_loader_test,  batch_size=8, shuffle=False, num_workers=1,pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "\n",
    "    return test_loader, num_nodes, seq_length, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "991c1bc0-5e2d-4ea2-9691-3e2fac259bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main_work(args):\n",
    "    \n",
    "    random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    \n",
    "    \n",
    "    # init metrics\n",
    "    best_acc1 = 0\n",
    "    best_roc  = 0\n",
    "    best_pr   = 0\n",
    "    best_f1   = 0\n",
    "    best_mcc  = 0\n",
    "    \n",
    "    best_test_acc1 = 0\n",
    "    best_test_roc  = 0\n",
    "    best_test_pr   = 0\n",
    "    best_test_f1   = 0\n",
    "    best_test_mcc  = 0    \n",
    "    \n",
    "    if args['tag'] == 'date':\n",
    "        local_date = time.strftime('%m.%d %H:%M', time.localtime(time.time()))\n",
    "        args['tag'] = local_date\n",
    "        \n",
    "    # Use the 'tag' which now contains either the date or a custom tag along with the dataset name for the directory\n",
    "    run_dir_name = f\"{args['dataset']}_{args['tag']}\"\n",
    "   \n",
    "    # Base directory for saving models\n",
    "    base_model_save_dir = \"saved_models\"\n",
    "   \n",
    "    # Specific directory for this run\n",
    "    specific_model_save_dir = os.path.join(base_model_save_dir, run_dir_name)\n",
    "    os.makedirs(specific_model_save_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Models will be saved in: {specific_model_save_dir}\")        \n",
    "\n",
    "\n",
    "    log_file = 'log/{}_gpu{}_{}_{}_exp.txt'.format(args['tag'], args['gpu'], args['arch'], args['dataset'])\n",
    "    \n",
    "    \n",
    "    if args['gpu'] is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args['gpu']))\n",
    "\n",
    "\n",
    "    # dataset\n",
    "    train_loader, val_loader, test_loader, num_nodes, seq_length, num_classes = get_default_train_val_test_loader(args)\n",
    "    \n",
    "    print('features / nodes', num_nodes,'total time graphs',args['groups'],'time series length',seq_length,'classes', num_classes)\n",
    "    \n",
    "    # training model from net.py\n",
    "    model = GNNStack(gnn_model_type=args['arch'], num_layers=args['num_layers'], \n",
    "                     groups=args['groups'], pool_ratio=args['pool_ratio'], kern_size=args['kern_size'], \n",
    "                     in_dim=args['in_dim'], hidden_dim=args['hidden_dim'], out_dim=args['out_dim'], \n",
    "                     seq_len=seq_length, num_nodes=num_nodes, num_classes=num_classes)\n",
    "\n",
    "    # print & log\n",
    "    log_msg('epochs {}, lr {}, weight_decay {}'.format(args['epochs'], args['lr'], args['weight_decay']), log_file)\n",
    "    \n",
    "    log_msg(str(args), log_file)\n",
    "\n",
    "\n",
    "    # determine whether GPU or not\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Warning! Using CPU!!!\")\n",
    "    elif args['gpu'] is not None:\n",
    "        torch.cuda.set_device(args['gpu'])\n",
    "\n",
    "        # collect cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model = model.cuda(args['gpu'])\n",
    "        if args['use_benchmark']:\n",
    "            cudnn.benchmark = True\n",
    "        print('Using cudnn.benchmark.')\n",
    "    else:\n",
    "        print(\"Error! We only have one gpu!!!\")\n",
    "\n",
    "   \n",
    "    criterion = nn.CrossEntropyLoss().cuda(args['gpu'])\n",
    "    \n",
    "    # TODO\n",
    "    # criterion = FocalLoss(gamma=0.1)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    \n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    # validation\n",
    "    if args['evaluate']:\n",
    "        validate(val_loader, model, criterion, args)\n",
    "        return\n",
    "\n",
    "    # train & valid\n",
    "    print('****************************************************')\n",
    "    print('Dataset: ', args['dataset'])\n",
    "\n",
    "    dataset_time = AverageMeter('Time', ':6.3f')\n",
    "\n",
    "    loss_train = []\n",
    "    acc_train = []\n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    epoches = []\n",
    "    \n",
    "    ########################\n",
    "    loss_test = []\n",
    "    acc_test  = []\n",
    "    #######################\n",
    "    \n",
    "    ###### more lists to have values\n",
    "    roc_train = []\n",
    "    pr_train  = []\n",
    "    f1_train  = []\n",
    "    mcc_train = []\n",
    "    \n",
    "    roc_val   = []\n",
    "    pr_val    = []\n",
    "    f1_val    = []\n",
    "    mcc_val   = []\n",
    "    ##################################################\n",
    "    roc_test   = []\n",
    "    pr_test    = []\n",
    "    f1_test    = []\n",
    "    mcc_test   = []     \n",
    "    #################################################\n",
    "    end = time.time()\n",
    "    for epoch in tqdm(range(args['epochs'])):\n",
    "    # for epoch in range(args['epochs']):\n",
    "        epoches += [epoch]\n",
    "\n",
    "        # train for one epoch\n",
    "        acc_train_per, loss_train_per, output_train_per, target_train_per = train(train_loader, model, criterion, optimizer, lr_scheduler, args)\n",
    "        \n",
    "        acc_train += [acc_train_per]\n",
    "        loss_train += [loss_train_per]\n",
    "        # calculate metric\n",
    "        # print(len(target_train_per),len(output_train_per))\n",
    "        auc_roc_value_train = roc_auc_score(target_train_per, output_train_per)\n",
    "        auc_pr_value_train = average_precision_score(target_train_per, output_train_per)\n",
    "        p2l_value_train = np.where(np.array(output_train_per) >= 0.5, 1, 0)\n",
    "        f1_value_train = f1_score(target_train_per, p2l_value_train.tolist())\n",
    "        mcc_value_train= matthews_corrcoef(target_train_per,p2l_value_train.tolist())\n",
    "        \n",
    "        #new code\n",
    "        roc_train += [auc_roc_value_train]\n",
    "        pr_train  += [auc_pr_value_train]\n",
    "        f1_train  += [f1_value_train]\n",
    "        mcc_train  += [mcc_value_train]\n",
    "\n",
    "        msg = f'TRAIN, epoch {epoch}, train_loss {loss_train_per}, train_acc {acc_train_per}, train_roc {auc_roc_value_train:.5f}, \\\n",
    "                train_pr {auc_pr_value_train:.5f}, train_f1 {f1_value_train:.5f},train_mcc {mcc_value_train:.5f}'\n",
    "\n",
    "        print(f'TRAIN, epoch {epoch}, train_loss {loss_train_per:.5f}, train_roc {auc_roc_value_train:.5f}, train_pr {auc_pr_value_train:.5f}, train_f1 {f1_value_train:.5f}, train_mcc {mcc_value_train:.5f}')\n",
    "        # tqdm.write(f'TRAIN, epoch {epoch}, train_loss {loss_train_per:.5f}, train_roc {auc_roc_value_train:.5f}, train_pr {auc_pr_value_train:.5f}, train_f1 {f1_value_train:.5f}, train_mcc {mcc_value_train:.5f}')\n",
    "        log_msg(msg, log_file)\n",
    "\n",
    "        \n",
    "        # evaluate on validation set\n",
    "        acc_val_per, loss_val_per, output_val_per, target_val_per = validate(val_loader, model, criterion, args)\n",
    "\n",
    "        acc_val  += [acc_val_per]\n",
    "        loss_val += [loss_val_per]\n",
    "        #calculate metric\n",
    "        # calculate metric\n",
    "        # print(len(target_val_per),len(output_val_per))\n",
    "        auc_roc_value_val = roc_auc_score(target_val_per, output_val_per)\n",
    "        auc_pr_value_val = average_precision_score(target_val_per, output_val_per)\n",
    "        p2l_value_val = np.where(np.array(output_val_per) >= 0.5, 1, 0)\n",
    "        f1_value_val = f1_score(target_val_per, p2l_value_val.tolist())\n",
    "        mcc_value_val= matthews_corrcoef(target_val_per,p2l_value_val.tolist())\n",
    "        #new code\n",
    "\n",
    "        msg = f'VAL, epoch {epoch}, val_loss {loss_val_per}, val_acc {acc_val_per}, val_roc {auc_roc_value_val:.5f}, val_pr {auc_pr_value_val:.5f},val_f1 {f1_value_val:.5f}, val_mcc {mcc_value_val:.5f}'\n",
    "        \n",
    "        print(f'VAL, epoch {epoch}, val_loss {loss_val_per:.5f}, val_roc {auc_roc_value_val:.5f}, val_pr {auc_pr_value_val:.5f},val_f1 {f1_value_val:.5f}, val_mcc {mcc_value_val:.5f}')\n",
    "        # tqdm.write(f'VAL, epoch {epoch}, val_loss {loss_val_per:.5f}, val_roc {auc_roc_value_val:.5f}, val_pr {auc_pr_value_val:.5f},val_f1 {f1_value_val:.5f}, val_mcc {mcc_value_val:.5f}')\n",
    "        log_msg(msg, log_file)\n",
    "        #########################################################################################################################\n",
    "        #########################################################################################################################\n",
    "        # remember best acc\n",
    "        best_acc1 = max(acc_val_per, best_acc1)\n",
    "        best_roc  = max(auc_roc_value_val, best_roc)\n",
    "        best_pr   = max(auc_pr_value_val, best_pr)\n",
    "        best_f1   = max(f1_value_val, best_f1)\n",
    "        best_mcc   = max(mcc_value_val, best_mcc)\n",
    "        \n",
    "        #########################################################################################################################\n",
    "\n",
    "        # Construct the filename with metrics\n",
    "        auc_roc_value_val_scalar = auc_roc_value_val.item() if isinstance(auc_roc_value_val, torch.Tensor) else auc_roc_value_val\n",
    "        auc_pr_value_val_scalar  = auc_pr_value_val.item() if isinstance(auc_pr_value_val, torch.Tensor) else auc_pr_value_val\n",
    "        f1_value_val_scalar      = f1_value_val.item() if isinstance(f1_value_val, torch.Tensor) else f1_value_val\n",
    "        mcc_value_val_scalar     = mcc_value_val.item() if isinstance(mcc_value_val, torch.Tensor) else mcc_value_val\n",
    "        #########################################################################################################################\n",
    "\n",
    "        # Now use these scalar values in the filename\n",
    "        filename = f\"model_epoch_{epoch}.pth\"\n",
    "       \n",
    "        # Continue with the existing logic to save the model\n",
    "        model_path = os.path.join(specific_model_save_dir, filename)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # measure elapsed time\n",
    "    dataset_time.update(time.time() - end)\n",
    "\n",
    "    # log & print the best_acc\n",
    "    msg = f'\\n\\n * BEST_ACC: {best_acc1}\\n * TIME: {dataset_time}\\n'\n",
    "    log_msg(msg, log_file)\n",
    "\n",
    "    print(f' * best_acc1: {best_acc1}, best_roc: {best_roc}, best_pr: {best_pr}, best_f1: {best_f1}, best_mcc: {best_mcc}')\n",
    "    print(f' * time: {dataset_time}')\n",
    "    print('****************************************************')\n",
    "\n",
    "\n",
    "    # collect cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, lr_scheduler, args):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc', ':6.2f')\n",
    "    # met_roc = AverageMeter('ROC', ':6.2f')\n",
    "    # met_pr = AverageMeter('PR', ':6.2f')\n",
    "    \n",
    "    output_list = []\n",
    "    target_list = [] \n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for count, (data, label, mask) in enumerate(train_loader):\n",
    "\n",
    "        # data in cuda\n",
    "        data = data.cuda(args['gpu']).type(torch.float)\n",
    "        data = data.view(data.size(0), 1, data.size(2), data.size(1))\n",
    "        mask = mask.cuda(args['gpu']).type(torch.bool)\n",
    "        label = label.cuda(args['gpu']).type(torch.long)\n",
    "\n",
    "        # compute output\n",
    "        output = model(data)\n",
    "        # print(len(output))\n",
    "        # print('output', output.shape, 'mask', mask.shape)\n",
    "        out_flat = torch.masked_select(output, mask.unsqueeze(-1)).reshape(-1, output.shape[-1])\n",
    "        # print(output)\n",
    "        # print(output.shape, mask.shape, out_flat.shape)\n",
    "        # break\n",
    "        # out_flat = torch.masked_select(output[:,:,1], mask)\n",
    "        \n",
    "\n",
    "        label_flat = torch.masked_select(label, mask)\n",
    "        # print('output',output.shape, 'mask', mask.shape,'out_flat', out_flat.shape, 'label', label.shape, 'label_flat', label_flat.shape)\n",
    "        loss = criterion(out_flat, label_flat)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1 = accuracy(out_flat, label_flat, topk=(1, 1))\n",
    "        \n",
    "        output_np = torch.softmax(out_flat, dim=1).detach().cpu().numpy()[:,1].tolist()\n",
    "        \n",
    "        target_np = label_flat.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        # print(output_np, target_np)\n",
    "        \n",
    "        losses.update(loss.item(), data.size(0))\n",
    "        top1.update(acc1[0], data.size(0))\n",
    "        \n",
    "        # met_roc.update(roc, data.size(0))\n",
    "        # met_pr.update(pr, data.size(0))\n",
    "        output_list += output_np\n",
    "        target_list += target_np\n",
    "\n",
    "        # compute gradient and do Adam step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    lr_scheduler.step(top1.avg)\n",
    "\n",
    "    return top1.avg, losses.avg, output_list, target_list\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, args):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    # met_roc = AverageMeter('ROC', ':6.2f')\n",
    "    # met_pr = AverageMeter('PR', ':6.2f')\n",
    "    output_list = []\n",
    "    target_list = [] \n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for count, (data, label, mask) in enumerate(val_loader):\n",
    "            if args['gpu'] is not None:\n",
    "                data = data.cuda(args['gpu'], non_blocking=True).type(torch.float)\n",
    "                data = data.view(data.size(0), 1, data.size(2), data.size(1))\n",
    "            if torch.cuda.is_available():\n",
    "                label = label.cuda(args['gpu'], non_blocking=True).type(torch.long)\n",
    "                mask  = mask.cuda(args['gpu'], non_blocking=True).type(torch.bool)\n",
    "\n",
    "            # compute output\n",
    "            output = model(data)\n",
    "            \n",
    "            out_flat = torch.masked_select(output, mask.unsqueeze(-1)).reshape(-1, output.shape[-1])\n",
    "            label_flat = torch.masked_select(label, mask)\n",
    "\n",
    "            loss = criterion(out_flat, label_flat)\n",
    "            \n",
    "            output_np = torch.softmax(out_flat, dim=1).detach().cpu().numpy()[:,1].tolist()\n",
    "            target_np = label_flat.detach().cpu().numpy().tolist()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1 = accuracy(out_flat, label_flat, topk=(1, 1))\n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            top1.update(acc1[0], data.size(0))\n",
    "            \n",
    "            output_list += output_np\n",
    "            target_list += target_np\n",
    "            \n",
    "            # met_roc.update(roc, data.size(0))\n",
    "            # met_pr.update(pr, data.size(0))\n",
    "\n",
    "    return top1.avg, losses.avg, output_list, target_list\n",
    "\n",
    "def load_model(model_path, model_class, *model_args, **model_kwargs):\n",
    "    \"\"\"\n",
    "    Load the model from a saved state dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path: Path to the saved model state dictionary.\n",
    "    - model_class: The class of the model to instantiate.\n",
    "    - model_args: Positional arguments for the model class instantiation.\n",
    "    - model_kwargs: Keyword arguments for the model class instantiation.\n",
    "\n",
    "    Returns:\n",
    "    - model: The loaded model ready for prediction.\n",
    "    \"\"\"\n",
    "    # Instantiate the model\n",
    "    model = model_class(*model_args, **model_kwargs)\n",
    "    # Load the saved state dictionary\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e238e65f-6835-43bb-be7c-8f7aa936d3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbbabaa-9dd4-49ae-be54-909c542b649e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_work(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51cb77-ecaa-48d1-8625-d397425b17f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58cefe-c901-4fbb-8fa2-e0a36b90e146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
